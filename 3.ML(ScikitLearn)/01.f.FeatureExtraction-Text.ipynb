{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Feature Extraction in Text</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Document :** It is simply a string representing a text.\n",
    "\n",
    "**Corpus :** It is a group of documents as list.\n",
    "\n",
    "\n",
    "|Document No.|X|Y|\n",
    "|----------------|-----------------|----------|\n",
    "|Doc1|fo@od is # good!|\t\tgood|\n",
    "|Doc2|food is tasty   |\t\tgood|\n",
    "|Doc3|quality is Good | \tgood|\n",
    "|Doc4|service is poor |\tnot good|\n",
    "|Doc5|it is too costly|\tnot good|\n",
    "|Doc6|cheap quality\t  |\tnot good|\n",
    " \n",
    "\n",
    "\n",
    "## <center>$Corpus=[Doc_1, Doc_2, Doc_3,Doc_4,...,Doc_n]$</center>\n",
    "\n",
    "## Sklearn Provides Two approaches for feature extraction\n",
    "    1) CountVectorizer\n",
    "    2) TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "### 1) CountVectorizer\n",
    "#### Working of CountVectorizer:\n",
    "|Docuent|Text|\n",
    "|------|---------------------------|\n",
    "|Doc1|'food is # good!    \\_@ 2019\"|\n",
    "|Doc2|'& Food # is * tasty'|\n",
    "|Doc3|'quality is Good'|\n",
    "|Doc4|'service is Poor poor means very poor'|\n",
    "|Doc5|'it is too costly'|\n",
    "|Doc6|'cheap quality'|\n",
    "\n",
    "\n",
    "**Step-1:Change all documents in lower case.**\n",
    "\n",
    "|Docuent|Text|\n",
    "|------|---------------------------|\n",
    "|Doc1|'food is # good! \\_@ 2019'|\n",
    "|Doc2|'& food # is * tasty'|\n",
    "|Doc3|'quality is good'|\n",
    "|Doc4|'service is poor poor means very poor'\n",
    "|Doc5|'it is too costly'\n",
    "|Doc6|'cheap quality'|\n",
    "\n",
    "**Step-2:Remove punctuation characters from all docs.**\n",
    "\n",
    "|Docuent|Text|\n",
    "|------|---------------------------|\n",
    "|Doc1|'food is good 2019'|\n",
    "|Doc2|'food is tasty'|\n",
    "|Doc3|'quality is good'|\n",
    "|Doc4|'service is poor poor means very poor'|\n",
    "|Doc5|'it is too costly'|\n",
    "|Doc6|'cheap quality'|\n",
    "\n",
    "**Step-3:Remove all single letter words.**\n",
    "\n",
    "|Docuent|Text|\n",
    "|------|---------------------------|\n",
    "|Doc1|'food is good 2019'|\n",
    "|Doc2|'food is tasty'|\n",
    "|Doc3|'quality is good'|\n",
    "|Doc4|'service is poor poor means very poor'|\n",
    "|Doc5|'it is too costly'|\n",
    "|Doc6|'cheap quality'|\n",
    "\n",
    "**Step-4:If stop_words argument is provided,remove all stop words from all docs.**\n",
    "\n",
    "|Docuent|Text|\n",
    "|------|---------------------------|\n",
    "|Doc1|'food good 2019'|\n",
    "|Doc2|'food tasty'|\n",
    "|Doc3|'quality good'|\n",
    "|Doc4|'service poor poor means poor'|\n",
    "|Doc5|'costly'|\n",
    "|Doc6|'cheap quality'|\n",
    "\n",
    "**Step-5:Collect unique words from corpus**\n",
    "\n",
    "<center>food,good,2019,tasty,quality,service,poor,means,cheap,costly</center>\n",
    "\n",
    "**Step-6:Arrange thsese words in natural order**\n",
    "\n",
    "These are feature names:-\n",
    "<center>2019,cheap,costly,food,good,means,poor,quality,service,tasty</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I food is # good! _@ 2019', '& Food # is * tasty', 'quality is Good', 'service is Poor poor means very poor', 'it is too costly', 'cheap quality']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "Text_dir=os.listdir('Text_Classification/Corpus')\n",
    "\n",
    "corpus=[]\n",
    "for a in Text_dir:\n",
    "    with open('Text_Classification/Corpus/'+a, 'r') as file:\n",
    "        filetext = file.read()\n",
    "        corpus.append(str(filetext))\n",
    "print(corpus)\n",
    "labels=['good','good','good','not good','not good','not good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1='I food is # good! _@ 2019'\n",
    "doc2='& Food # is * tasty'\n",
    "doc3='quality is Good'\n",
    "doc4='service is Poor poor means very poor'\n",
    "doc5='it is too costly'\n",
    "doc6='cheap quality'\n",
    "\n",
    "corpus=[doc1,doc2,doc3,doc4,doc5,doc6]\n",
    "labels=['good','good','good','not good','not good','not good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(stop_words='english',binary=True)\n",
    "cv.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2019',\n",
       " 'cheap',\n",
       " 'costly',\n",
       " 'food',\n",
       " 'good',\n",
       " 'means',\n",
       " 'poor',\n",
       " 'quality',\n",
       " 'service',\n",
       " 'tasty']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step-7:For each doc count frequency of these features**\n",
    "\n",
    "- If Binary is False than count frequency\n",
    "- else : Single frequency Take\n",
    "\n",
    "|Doc |2019 | cheap|costly|food| good|means|poor|quality|service|tasty|\n",
    "|----|-----|------|------|----|-----|-----|----|-------|-------|-----|\n",
    "|Doc1|\t  1|     0|\t    0|   1|\t   1|    0|   0|      0|      0|    0|\n",
    "|Doc2|\t  0|     0|     0|   1|    0|    0|   0|      0|      0|    1|\n",
    "|Doc3|\t  0|     0|     0|   0|    1|    0|   0|      1|      0|    0|\n",
    "|Doc4|\t  0|     0|     0|   0|    0|    1|   3|      0|      1|   \t0|\n",
    "|Doc5|\t  0|     0|     1|   0|    0|    0|   0|      0|      0|    0|\n",
    "|Doc6|    0|     1|     0|   0|    0|    0|   0|      1|      0|    0|\n",
    "\n",
    "**Step-8:Tranform(corpus) returns 2d arrays of this corpus**\n",
    "\n",
    "Note:bydefault CV counts frequency of feature in each doc and\n",
    "we may also use CV to check only absence and presence of feature\n",
    "\n",
    "cv=CountVectorizer(stop_word='english',binary=True)\n",
    "\t\n",
    "\t   [[1, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
    "        [0, 0, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 1, 1, 0, 1, 0],\n",
    "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0, 0, 1, 0, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 9)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 7)\t1\n",
      "  (3, 5)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 8)\t1\n",
      "  (4, 2)\t1\n",
      "  (5, 1)\t1\n",
      "  (5, 7)\t1\n"
     ]
    }
   ],
   "source": [
    "print(cv.transform(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 1, 0, 1, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform(corpus).todense() # Convert 2D matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2019</th>\n",
       "      <th>cheap</th>\n",
       "      <th>costly</th>\n",
       "      <th>food</th>\n",
       "      <th>good</th>\n",
       "      <th>means</th>\n",
       "      <th>poor</th>\n",
       "      <th>quality</th>\n",
       "      <th>service</th>\n",
       "      <th>tasty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      2019  cheap  costly  food  good  means  poor  quality  service  tasty\n",
       "Doc1     1      0       0     1     1      0     0        0        0      0\n",
       "Doc2     0      0       0     1     0      0     0        0        0      1\n",
       "Doc3     0      0       0     0     1      0     0        1        0      0\n",
       "Doc4     0      0       0     0     0      1     1        0        1      0\n",
       "Doc5     0      0       1     0     0      0     0        0        0      0\n",
       "Doc6     0      1       0     0     0      0     0        1        0      0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy  as np\n",
    "a=np.array(cv.get_feature_names(),dtype='str')\n",
    "b=np.array(cv.transform(corpus).todense())\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(b,columns=a,index=['Doc1','Doc2','Doc3','Doc4','Doc5','Doc6'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**get stop Words**\n",
    "\n",
    "<p>\n",
    "[among, me, than, thereupon, cannot, yours, became, about, you, off, toward, enough, yet, fire, an, in, sincere, either, anyhow, behind, anyway, seem, up, herein, also, indeed, or, beside, thru, therefore, whereby, here, several, seemed, beyond, detail, none, thin, wherein, yourself, describe, find, were, thereby, to, do, formerly, my, as, herself, now, latter, front, fifty, but, why, everything, itself, most, nothing, hereupon, she, due, others, ltd, any, elsewhere, since, whereas, re, even, however, moreover, must, rather, amongst, move, bottom, yourselves, besides, being, that, twenty, another, system, such, around, if, some, perhaps, somehow, within, this, anywhere, all, few, see, cant, third, already, cry, ever, part, out, someone, hasnt, whose, become, himself, four, without, five, back, eg, while, whether, always, empty, his, where, after, amount, further, then, hence, of, except, take, your, he, on, whereupon, nobody, might, its, onto, give, call, next, wherever, six, would, afterwards, him, into, against, hundred, thence, thereafter, whence, fifteen, myself, ie, again, hereafter, throughout, between, though, something, otherwise, therein, fill, above, seeming, often, whither, anything, although, during, eight, until, bill, am, meanwhile, nevertheless, i, ourselves, both, upon, it, whatever, etc, con, may, almost, at, through, mill, everywhere, same, much, not, no, along, whoever, only, her, well, done, neither, before, has, mine, name, becomes, becoming, put, once, under, sometime, less, could, more, nine, how, many, one, thick, eleven, themselves, please, ours, mostly, below, these, de, co, never, made, sixty, top, thus, every, three, inc, via, found, too, other, those, them, there, each, last, forty, be, down, had, whole, they, because, amoungst, so, is, per, whom, couldnt, hers, interest, towards, nor, somewhere, un, with, us, least, for, and, side, have, by, still, a, can, the, namely, whereafter, anyone, when, show, full, noone, nowhere, who, own, should, was, been, over, go, their, across, keep, will, what, else, together, two, get, we, former, which, everyone, beforehand, alone, are, ten, twelve, hereby, our, whenever, latterly, seems, sometimes, from, very, first, serious]</p>\n",
    "\n",
    "**Punctuation:**\n",
    "\n",
    "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(cv.get_stop_words()))\n",
    "'very' in cv.get_stop_words()    #True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Tf idf Vectorizer\n",
    "- Tf means **term-frequency** while tf–idf means term-frequency times **inverse document-frequency**:\n",
    "## <center>$\\text{tf-idf(t,d)}=\\text{tf(t,d)} \\times \\text{idf(t)}$ </center>\n",
    "\n",
    "- Using the TfidfTransformer’s default settings, TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False) the term frequency, the number of times a term occurs in a given document, is multiplied with idf component, which is computed as\n",
    "## <center>$\\text{idf}(t) = \\log{\\frac{1 + n}{1+\\text{df}(t)}} + 1$ </center>\n",
    "\n",
    "- where $n$ is the total number of documents in the document set, and ${df}(t)$ is the number of documents in the document set that contain term . The resulting tf-idf vectors are then normalized by the Euclidean norm\n",
    "\n",
    "## <center>$v_{norm} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v{_1}^2 +v{_2}^2 + \\dots + v{_n}^2}}$ </center>\n",
    "\n",
    "\n",
    "This was originally a term weighting scheme developed for information retrieval (as a ranking function for\n",
    "search engines results) that has also found good use in document classification and clustering.\n",
    "\n",
    "**Example of tf–idf**\n",
    "\n",
    "- Suppose that we have term count tables of a corpus consisting of only two documents, as listed on the right.\n",
    "\n",
    "Document 1\n",
    "\n",
    "|Term\t|Term Count|\n",
    "|---------|----------|\n",
    "|this\t|1|\n",
    "|is\t|1|\n",
    "|a\t|2|\n",
    "|sample|1|\n",
    "\n",
    "Document 2\n",
    "\n",
    "|Term|\tTerm Count|\n",
    "|------|----------|\n",
    "|this|\t1|\n",
    "|is\t|1|\n",
    "|another|\t2|\n",
    "|example|3|\n",
    "\n",
    "The calculation of tf–idf for the term \"this\" is performed as follows:\n",
    "\n",
    "In its raw frequency form, tf is just the frequency of the \"this\" for each document. In each document, the word \"this\" appears once; but as the document 2 has more words, its relative frequency is smaller.\n",
    "\n",
    "<center>${\\displaystyle \\mathrm {tf} ({\\mathsf {''this''}},d_{1})={\\frac {1}{5}}=0.2}$</center>\n",
    "\n",
    "<center>${\\displaystyle \\mathrm {tf} ({\\mathsf {''this''}},d_{2})={\\frac {1}{7}}\\approx 0.14}$</center>\n",
    "\n",
    "An idf is constant per corpus, and accounts for the ratio of documents that include the word \"this\". In this case, we have a corpus of two documents and all of them include the word \"this\".\n",
    "\n",
    "<center>${\\displaystyle \\mathrm {idf} ({\\mathsf {''this''}},D)=\\log \\left({\\frac {2}{2}}\\right)=0}$</center>\n",
    "\n",
    "So tf–idf is zero for the word \"this\", which implies that the word is not very informative as it appears in all documents.\n",
    "\n",
    "<center>${\\displaystyle \\mathrm {tfidf} ({\\mathsf {''this''}},d_{1},D)=0.2\\times 0=0} $</center>\n",
    "\n",
    "<center>${\\displaystyle \\mathrm {tfidf} ({\\mathsf {''this''}},d_{1},D)=0.2\\times 0=0}$</center>\n",
    "\n",
    "The word \"example\" is more interesting - it occurs three times, but only in the second document:\n",
    "\n",
    "<center>${\\displaystyle \\mathrm {tf} ({\\mathsf {''example''}},d_{1})={\\frac {0}{5}}=0}$</center>\n",
    "\n",
    "<center>${\\displaystyle \\mathrm {tf} ({\\mathsf {''example''}},d_{2})={\\frac {3}{7}}\\approx 0.429} $</center>\n",
    "\n",
    "<center>${\\displaystyle \\mathrm {idf} ({\\mathsf {''example''}},D)=\\log \\left({\\frac {2}{1}}\\right)=0.301}$</center>\n",
    "\n",
    "Finally,\n",
    "\n",
    "<center>${\\displaystyle \\mathrm {tfidf} ({\\mathsf {''example''}},d_{1},D)=\\mathrm {tf} ({\\mathsf {''example''}},d_{1})\\times \\mathrm {idf} ({\\mathsf {''example''}},D)=0\\times 0.301=0}$</center>\n",
    "\n",
    "<center>${\\displaystyle \\mathrm {tfidf} ({\\mathsf {''example''}},d_{2},D)=\\mathrm {tf} ({\\mathsf {''example''}},d_{2})\\times \\mathrm {idf} ({\\mathsf {''example''}},D)=0.429\\times 0.301\\approx 0.129}$</center>\n",
    "\n",
    "(using the base 10 logarithm).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working of TfidfVectorizer:(Term Frequency Inverse Document Frequency)\n",
    "\n",
    "\n",
    "**Step-1:Change all documents in lower case.**\n",
    "\n",
    "**Step-2:Remove punctuation characters from all docs.**\n",
    "\n",
    "**Step-3:Remove all single letter words.**\n",
    "\n",
    "**Step-4:If stop_words argument is provided,remove all stop words from all docs.**\n",
    "\n",
    "**Step-5:Collect unique words from corpus**\n",
    "\n",
    "**Step-6:Arrange thsese words in natural order**\n",
    "\n",
    "**step-7:for each word find out it's term frequency and inverse document frequency**\n",
    "\n",
    "|Docuent|Text|\n",
    "|------|---------------------------|\n",
    "|Doc1|'food good 2019'|\n",
    "|Doc2|'food tasty'|\n",
    "|Doc3|'quality good'|\n",
    "|Doc4|'service poor poor means poor'|\n",
    "|Doc5|'costly'|\n",
    "|Doc6|'cheap quality'|\n",
    "\n",
    "|Doc |totalword|2019 | cheap|costly|food| good|means|poor|quality|service|tasty|\n",
    "|----|---------|-----|------|------|----|-----|-----|----|-------|-------|-----|\n",
    "|Doc1|\t      3|  1|     0|\t    0|   1|\t   1|    0|   0|      0|      0|    0|\n",
    "|Doc2|\t      2|  0|     0|     0|   1|    0|    0|   0|      0|      0|    1|\n",
    "|Doc3|\t      2|  0|     0|     0|   0|    1|    0|   0|      1|      0|    0|\n",
    "|Doc4|\t      5|  0|     0|     0|   0|    0|    1|   3|      0|      1|   \t0|\n",
    "|Doc5|\t      1|  0|     0|     1|   0|    0|    0|   0|      0|      0|    0|\n",
    "|Doc6|        2|  0|     1|     0|   0|    0|    0|   0|      1|      0|    0|\n",
    "\n",
    " idf(d, t) = log [ (1 + n) / (1 + df(d, t)) ] + 1.\n",
    "\n",
    "**TF:**\n",
    "<center>${\\displaystyle \\mathrm {tf} ({\\mathsf {''2019''}},d_{1})={\\frac {1}{3}}\\approx0.333333}$</center>\n",
    "<center>${\\displaystyle \\mathrm {tf} ({\\mathsf {''2019''}},d_{2})={\\frac {0}{2}}= 0}$</center>\n",
    "<center>${\\displaystyle \\mathrm {tf} ({\\mathsf {''2019''}},d_{3})={\\frac {0}{2}}= 0}$</center>\n",
    "<center>${\\displaystyle \\mathrm {tf} ({\\mathsf {''2019''}},d_{4})={\\frac {0}{5}}= 0}$</center>\n",
    "<center>${\\displaystyle \\mathrm {tf} ({\\mathsf {''2019''}},d_{5})={\\frac {0}{1}}= 0}$</center>\n",
    "<center>${\\displaystyle \\mathrm {tf} ({\\mathsf {''2019''}},d_{6})={\\frac {0}{2}}= 0}$</center>\n",
    "\n",
    "**idf:**\n",
    "<center>${\\displaystyle \\mathrm {idf} ({\\mathsf {''2019''}},D)=\\log \\left({\\frac {6}{1}}\\right)\\approx 0.77815125038}$</center>\n",
    "\n",
    "**Tf_idf=tf\\*idf**\n",
    "<center>${\\displaystyle \\mathrm {tfidf} ({\\mathsf {''2019''}},d_{1},D)=\\mathrm {tf} ({\\mathsf {''2019''}},d_{1})\\times \\mathrm {idf} ({\\mathsf {''2019''}},D)=0\\times 0.301=0}$</center>\n",
    "\n",
    "<center>${\\displaystyle \\mathrm {tfidf} ({\\mathsf {''2019''}},d_{2},D)=\\mathrm {tf} ({\\mathsf {''2019''}},d_{2})\\times \\mathrm {idf} ({\\mathsf {''2019''}},D)=0\\times 0.301=0}$</center>\n",
    "\n",
    "<center>${\\displaystyle \\mathrm {tfidf} ({\\mathsf {''2019''}},d_{3},D)=\\mathrm {tf} ({\\mathsf {''2019''}},d_{3})\\times \\mathrm {idf} ({\\mathsf {''2019''}},D)=0\\times 0.301=0}$</center>\n",
    "\n",
    "<center>${\\displaystyle \\mathrm {tfidf} ({\\mathsf {''2019''}},d_{4},D)=\\mathrm {tf} ({\\mathsf {''2019''}},d_{4})\\times \\mathrm {idf} ({\\mathsf {''2019''}},D)=0\\times 0.301=0}$</center>\n",
    "\n",
    "<center>${\\displaystyle \\mathrm {tfidf} ({\\mathsf {''2019''}},d_{5},D)=\\mathrm {tf} ({\\mathsf {''2019''}},d_{5})\\times \\mathrm {idf} ({\\mathsf {''2019''}},D)=0\\times 0.301=0}$</center>\n",
    "\n",
    "<center>${\\displaystyle \\mathrm {tfidf} ({\\mathsf {''2019''}},d_{6},D)=\\mathrm {tf} ({\\mathsf {''2019''}},d_{6})\\times \\mathrm {idf} ({\\mathsf {''2019''}},D)=0\\times 0.301=0}$</center>\n",
    "\n",
    "**step-8:multiply tf and idf of this word :**\n",
    "\n",
    "\n",
    "| Doc| 2019  | cheap | costly | food  | good  | means | poor  | quality | service | tasty  |\n",
    "|----|-------|-------|--------|-------|-------|-------|-------|---------|---------|--------|\n",
    "|Doc1|0\n",
    "|Doc2|\t\n",
    "|Doc3|\t\n",
    "|Doc4|\t\n",
    "|Doc5|\t\n",
    "|Doc6|  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc1='I food is # good! _@ 2019'\n",
    "Doc2='& Food # is * tasty'\n",
    "Doc3='quality is Good'\n",
    "Doc4='service is Poor poor means very poor'\n",
    "Doc5='it is too costly'\n",
    "Doc6='cheap quality'\n",
    "\n",
    "Corpus=[Doc1,Doc2,Doc3,Doc4,Doc5,Doc6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv=TfidfVectorizer(stop_words='english')\n",
    "tv.fit(Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2019',\n",
       " 'cheap',\n",
       " 'costly',\n",
       " 'food',\n",
       " 'good',\n",
       " 'means',\n",
       " 'poor',\n",
       " 'quality',\n",
       " 'service',\n",
       " 'tasty']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t0.5355058021985527\n",
      "  (0, 3)\t0.5355058021985527\n",
      "  (0, 0)\t0.6530444637414585\n",
      "  (1, 9)\t0.7732623667832087\n",
      "  (1, 3)\t0.6340862024337309\n",
      "  (2, 7)\t0.7071067811865476\n",
      "  (2, 4)\t0.7071067811865476\n",
      "  (3, 8)\t0.30151134457776363\n",
      "  (3, 6)\t0.9045340337332909\n",
      "  (3, 5)\t0.30151134457776363\n",
      "  (4, 2)\t1.0\n",
      "  (5, 7)\t0.6340862024337309\n",
      "  (5, 1)\t0.7732623667832087\n"
     ]
    }
   ],
   "source": [
    "print(tv.transform(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.65304446, 0.        , 0.        , 0.5355058 , 0.5355058 ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.6340862 , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.77326237],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.70710678,\n",
       "         0.        , 0.        , 0.70710678, 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.30151134, 0.90453403, 0.        , 0.30151134, 0.        ],\n",
       "        [0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.77326237, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.6340862 , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv.transform(corpus).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2019</th>\n",
       "      <th>cheap</th>\n",
       "      <th>costly</th>\n",
       "      <th>food</th>\n",
       "      <th>good</th>\n",
       "      <th>means</th>\n",
       "      <th>poor</th>\n",
       "      <th>quality</th>\n",
       "      <th>service</th>\n",
       "      <th>tasty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc0</th>\n",
       "      <td>0.653044</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.535506</td>\n",
       "      <td>0.535506</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.634086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.773262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.301511</td>\n",
       "      <td>0.904534</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.301511</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.773262</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.634086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          2019     cheap  costly      food      good     means      poor  \\\n",
       "Doc0  0.653044  0.000000     0.0  0.535506  0.535506  0.000000  0.000000   \n",
       "Doc1  0.000000  0.000000     0.0  0.634086  0.000000  0.000000  0.000000   \n",
       "Doc2  0.000000  0.000000     0.0  0.000000  0.707107  0.000000  0.000000   \n",
       "Doc3  0.000000  0.000000     0.0  0.000000  0.000000  0.301511  0.904534   \n",
       "Doc4  0.000000  0.000000     1.0  0.000000  0.000000  0.000000  0.000000   \n",
       "Doc5  0.000000  0.773262     0.0  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       quality   service     tasty  \n",
       "Doc0  0.000000  0.000000  0.000000  \n",
       "Doc1  0.000000  0.000000  0.773262  \n",
       "Doc2  0.707107  0.000000  0.000000  \n",
       "Doc3  0.000000  0.301511  0.000000  \n",
       "Doc4  0.000000  0.000000  0.000000  \n",
       "Doc5  0.634086  0.000000  0.000000  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy  as np\n",
    "a=np.array(tv.get_feature_names(),dtype='str')\n",
    "b=np.array(tv.transform(corpus).todense())\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(b,columns=a,index=['Doc0','Doc1','Doc2','Doc3','Doc4','Doc5'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.shape (6, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.25276297, 2.25276297, 2.25276297, 1.84729786, 1.84729786,\n",
       "       2.25276297, 2.25276297, 1.84729786, 2.25276297, 2.25276297])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
