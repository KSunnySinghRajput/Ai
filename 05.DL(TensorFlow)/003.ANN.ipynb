{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hSDcY116vMuh"
   },
   "source": [
    "# <center>Artificial Neural Network</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hSDcY116vMuh"
   },
   "source": [
    "A single perceptron (or neuron) can be imagined as a Logistic Regression. Artificial Neural Network, or ANN, is a group of multiple perceptrons/ neurons at each layer. ANN is also known as a **Feed-Forward Neural network** because inputs are processed only in the forward direction:\n",
    "\n",
    "![](_pic/ANN/ANN-Graph.webp)\n",
    "<p style='text-align:right'></p>\n",
    "As you can see here, ANN consists of 3 layers – Input, Hidden and Output. The input layer accepts the inputs, the hidden layer processes the inputs, and the output layer produces the result. Essentially, each layer tries to learn certain weights.\n",
    "\n",
    "ANN can be used to solve problems related to:\n",
    "\n",
    "- Tabular data\n",
    "- Image data\n",
    "- Text data\n",
    "\n",
    "**Advantages of Artificial Neural Network (ANN)**\n",
    "\n",
    "Artificial Neural Network is capable of learning any nonlinear function. Hence, these networks are popularly known as Universal Function Approximators. ANNs have the capacity to learn weights that map any input to the output.\n",
    "\n",
    "One of the main reasons behind universal approximation is the activation function. Activation functions introduce nonlinear properties to the network. This helps the network learn any complex relationship between input and output.\n",
    "![](_pic/ANN/13UpdymQx-C1tBKRnfD7eOg.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hSDcY116vMuh"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ovb-s66IvMvV"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feed Forward Neural Networks\n",
    "\n",
    "![](_pic/ANN/2layerann.png)\n",
    "\n",
    "*<p style='text-align:right'>By Mcstrother (Own work) [CC BY 3.0], via Wikimedia Commons</p>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Softmax Output**\n",
    "\n",
    "![](_pic/ANN/softmax.png)\n",
    "<p style='text-align:right'>By Lizhongzheng (Own work) [CC BY-SA 4.0], via Wikimedia Commons</p>\n",
    "\n",
    "<p style='text-align:center'> $\\sum_{i=0}^{9} p_i = 1$ </p>\n",
    "\n",
    "<p style='text-align:center'> $\\hat y = \\Big[$ p0 p1 p2 p3 p4 p5 p6 p7 p8 p9 $\\Big]$ </p>\n",
    "\n",
    "- probability distribution - how confident am I of this prediction?\n",
    "- neurons in the softmax layer are interdependent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradient Descent\n",
    "\n",
    "![](_pic/ANN/grdesc.png)\n",
    "*<p style='text-align:right'>By Olegalexandrov and Zerodamage [Public domain], via Wikimedia Commons</p>*\n",
    "\n",
    "- purpose of training a neural net: determine the best weights\n",
    "- Gradient Descent: find the minimum of a function\n",
    "- in math - gradient is a generalization of the derivative\n",
    "\n",
    "- derivative - function of a variable\n",
    "- gradient - function of a set of variables\n",
    "\n",
    "![](_pic/ANN/mountain.jpg)\n",
    "*\"You need a some kind of heuristic to find the correct way. One of the best choice and the simplest one is to check the decline rate by single step for each possible direction and choose the most steepest one.\"*\n",
    "\n",
    "<p style='text-align:right'>(Eren Golge https://www.quora.com/What-is-an-intuitive-explanation-of-gradient-descent)</p>\n",
    "\n",
    "- learning rate - too small or too high might be problematic\n",
    "\n",
    "![](_pic/ANN/grdesc2.jpeg)\n",
    "\n",
    "*<p style='text-align:right'>GD with small and large learning rates. By Andrew Ng’s via ML course on Coursera</p>*\n",
    "### 4.1. Steps with GD:\n",
    "\n",
    "-   (1). Random initialization of weights. Find the gradient at current position.\n",
    "-   (2). Take a step in the direction of the steepest descent.\n",
    "-   (3). Determine the gradient at this new position. Take a step in the direction of the steepest descent. \n",
    "-   (4). Repeat until arriving at the point of the minimum error.\n",
    "\n",
    "\n",
    "- Variations of GD: batch GD, stochastic GD (SGD), mini-batch GD.\n",
    "- Optimizations: Momentum, RMSprop, Adam, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Backpropagation\n",
    "\n",
    "- forward pass (X, W, b) => Z, A => $\\hat y$\n",
    "\n",
    "![](_pic/ANN/forwardp.png)\n",
    "\n",
    "*<p style='text-align:right'>Forward Pass. By Sebastian Raschka via Github</p>*\n",
    "\n",
    "## - $\\hat y$ vs y => error (cost)\n",
    "\n",
    "In backpropagation:\n",
    "\n",
    "- we propagate the error backwards => update weights => reduce error (Chain Rule, Power Rule)\n",
    "\n",
    "![](_pic/ANN/backprop.png)\n",
    "\n",
    "*<p style='text-align:right'>Backward Pass. By Sebastian Raschka via Github</p>*\n",
    "\n",
    "### Backpropagation - determines the gradient of the error function with respect to the weights:\n",
    "\n",
    "- (1). Determine the gradient of the final layer of weights\n",
    "- (2). Determine the gradient of the hidden layer of weights\n",
    "- ...\n",
    "- (n). Determine the gradient of the initial layer of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "01.NNT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
