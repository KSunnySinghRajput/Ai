{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "embedded-employee",
   "metadata": {},
   "source": [
    "# <center>Perceptron Learning Algorithm</center>\n",
    "## Perceptron\n",
    "- The perceptron is a single processing unit of any neural network. \n",
    "- **Frank Rosenblatt** first proposed in **1958** is a simple neuron which is used to classify its input into one or two categories. \n",
    "- Perceptron is a linear classifier, and is used in supervised learning. It helps to organize the given input data.\n",
    "\n",
    "A perceptron is a neural network unit that does a precise computation to detect features in the input data. Perceptron is mainly used to classify the data into two parts. Therefore, it is also known as **Linear Binary Classifier**.\n",
    "\n",
    "## The perceptron consists of 4 parts.\n",
    "![](_pic/ANN/single-layer-perceptron-in-tensorflow2.png)\n",
    "### <p style='text-align:center'> $z = âˆ‘_{i}^{i=n}w_{i}*x_{i}$</p>\n",
    "#### The output is a function of z:\n",
    "### <p style='text-align:center'> $y = f(z)$ </p>\n",
    "### Output:\n",
    "## <p style='text-align:center; color:blue'> $y = f(X*W+b)$ </p>\n",
    "- **1.Input value or One input layer:** The input layer of the perceptron is made of artificial input neurons and takes the initial data into the system for further processing.\n",
    "- **2.Weights and Bias:**\n",
    "    - **Weight:** It represents the dimension or strength of the connection between units. If the weight to node 1 to node 2 has a higher quantity, then neuron 1 has a more considerable influence on the neuron.\n",
    "    - **Bias:** It is the same as the intercept added in a linear equation. It is an additional parameter which task is to modify the output along with the weighted sum of the input to the other neuron.\n",
    "- **3.Net sum:** It calculates the total sum.\n",
    "- **4.Activation Function:** A neuron can be activated or not, is determined by an activation function. The activation function calculates a weighted sum and further adding bias with it to give the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "divine-palestine",
   "metadata": {},
   "source": [
    "### 4. [Activation Function](ActivatioFunction.ipynb) :\n",
    "**1. The Sigmoid Function**\n",
    "\n",
    "![](_pic/AF/sigmoid.png)\n",
    "<p style='text-align:right'>By MartinThoma (Own work) [CC0], via Wikimedia Commons</p>\n",
    "<p style='text-align:center'> $f(z) = \\frac{1}{1+e^{-z}}$ </p>\n",
    "\n",
    "**2. The Hyperbolic Tangent**\n",
    "![](_pic/AF/tanh.png)\n",
    "<p style='text-align:right'>By Laughsinthestocks (Own work) [CC BY-SA 4.0], via Wikimedia Commons</p>\n",
    "<p style='text-align:center'> $f(z) = tanh(z)$ </p>\n",
    "<p style='text-align:center'> $f(z) = \\frac{2}{1+e^{-2z}} - 1$ </p>\n",
    "\n",
    "**3. The Rectified Linear Unit (ReLU)**\n",
    "\n",
    "![](_pic/AF/relu.png)\n",
    "<p style='text-align:right'>By Laughsinthestocks (Own work) [CC BY-SA 4.0], via Wikimedia Commons</p>\n",
    "\n",
    "<p style='text-align:center'> $f(z) = max(0,z)$ </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "legal-transparency",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals \n",
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-trainer",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "determined-declaration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf Graph\n",
    "def NN_Nod(x, w, b):\n",
    "    product = tf.matmul(x, w)\n",
    "    x = tf.add(x,product)\n",
    "    return x\n",
    "# Create a `Function` object that contains a graph\n",
    "P_Layer = tf.function(NN_Node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "crucial-maldives",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex1\n",
    "# tf Graph Input\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "x1 = tf.constant([[1.0, 2.0]])\n",
    "y1 = tf.constant([[2.0], [3.0]])\n",
    "b1 = tf.constant(4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "suspected-mouse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It just works!\n",
    "P_Layer(x1, y1, b1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "coral-afternoon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "## Ex4\n",
    "tf.compat.v1.disable_eager_execution() # need to disable eager in TF2.x\n",
    "# Build a graph.\n",
    "a = tf.constant(5.0)\n",
    "b = tf.constant(6.0)\n",
    "c = a * b\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.compat.v1.Session()\n",
    "\n",
    "# Evaluate the tensor `c`.\n",
    "print(sess.run(c)) # prints 30.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "worldwide-discharge",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-18662ed1baae>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-18662ed1baae>\"\u001b[1;36m, line \u001b[1;32m25\u001b[0m\n\u001b[1;33m    - (1). constructing the graph: tf.constant, tf.placeholder, tf.variable, tf.nn.sigmoid, tf.matmul, etc.\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "x_data = np.random.randn(4,9) # 4 row, 6 Coloum  -> Feature 6\n",
    "w_data = np.random.randn(9,1)\n",
    "\n",
    "x = tf.placeholder(tf.float32, name='X', shape=(4,9))\n",
    "w = tf.placeholder(tf.float32, name='W', shape=(9,1))\n",
    "b = tf.fill((4,1), -1., name='bias')\n",
    "y = tf.matmul(x,w)+b\n",
    "s = tf.reduce_max(y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    out_p = sess.run(s, feed_dict={x:x_data, w:w_data})\n",
    "    \n",
    "print('Output: {}'.format(out_p))\n",
    "\n",
    "x_data = np.random.randn(4,9)\n",
    "w_data = np.random.randn(9,1)\n",
    "\n",
    "x = tf.placeholder(tf.float32, name='X', shape=(4,9))\n",
    "w = tf.placeholder(tf.float32, name='W', shape=(9,1))\n",
    "\n",
    "b = tf.fill((4,1), -1., name='bias')\n",
    "y = tf.matmul(x,w)+b\n",
    "s = tf.reduce_max(y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    out_p = sess.run(s, feed_dict={x:x_data, w:w_data})\n",
    "print(x_data)\n",
    "print(w_data)\n",
    "print('Output: {}'.format(out_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-actress",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-emerald",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-locking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-pride",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-vatican",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "integrated-roads",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "modified-federation",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-e9e9c42230f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# X, Y, W, b\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'W'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [
    "# X, Y, W, b\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y_true = tf.placeholder(tf.float32, shape=None)\n",
    "\n",
    "w = tf.Variable([[0,0,0]], dtype=tf.float32, name='W')\n",
    "b = tf.Variable(0, dtype=tf.float32, name='b')\n",
    "\n",
    "# Output\n",
    "y_pred = tf.matmul(w, tf.transpose(x))+b\n",
    "# MSE (Mean Squared Error)\n",
    "loss = tf.reduce_mean(tf.square(y_true-y_pred))\n",
    "\n",
    "# Cross Entropy\n",
    "\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-flour",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-newsletter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04.Introduction to Gradients and Automatic Differentiation\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  y = x**2\n",
    "\n",
    "# dy = 2x * dx\n",
    "dy_dx = tape.gradient(y, x)\n",
    "dy_dx.numpy()\n",
    "\n",
    "w = tf.Variable(tf.random.normal((3, 2)), name='w')\n",
    "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
    "x = [[1., 2., 3.]]\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  y = x @ w + b\n",
    "  loss = tf.reduce_mean(y**2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
